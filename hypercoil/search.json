[
  {
    "objectID": "api/nn.html",
    "href": "api/nn.html",
    "title": "nn",
    "section": "",
    "text": "loss.nn\nLoss functions as parameterised, callable functional objects.\n\n\n\n\n\nName\nDescription\n\n\n\n\nBatchCorrelationLoss\nBatch correlation loss.\n\n\nBimodalSymmetricLoss\nLoss based on the minimum distance from either of two modes.\n\n\nBregmanDivergenceLogitLoss\nLoss based on the Bregman divergence between two categorical distributions.\n\n\nBregmanDivergenceLoss\nLoss based on the Bregman divergence between two categorical distributions.\n\n\nCompactnessLoss\nLoss function penalising distances between locations in a mass and the centroid of that mass.\n\n\nConnectopyLoss\nGeneralised connectopic functional, for computing different kinds of connectopic maps.\n\n\nConstraintViolationLoss\nLoss function for constraint violations.\n\n\nDispersionLoss\nLoss function penalising proximity between vectors.\n\n\nEigenmapsLoss\nLaplacian eigenmaps functional.\n\n\nEntropyLogitLoss\nLoss based on the entropy of a categorical distribution.\n\n\nEntropyLoss\nLoss based on the entropy of a categorical distribution.\n\n\nEquilibriumLogitLoss\nMass equilibrium loss.\n\n\nEquilibriumLoss\nMass equilibrium loss.\n\n\nFunctionalHomogeneityLoss\nGlobal functional homogeneity loss.\n\n\nGramDeterminantLoss\nLoss based on the determinant of the Gram matrix.\n\n\nGramLogDeterminantLoss\nLoss based on the log-determinant of the Gram matrix.\n\n\nHingeLoss\nHinge loss function.\n\n\nInterhemisphericTetherLoss\nLoss function penalising distance between matched parcels or objects on opposite hemispheres.\n\n\nJSDivergenceLogitLoss\nLoss based on the Jensen-Shannon divergence between two categorical distributions.\n\n\nJSDivergenceLoss\nLoss based on the Jensen-Shannon divergence between two categorical distributions.\n\n\nKLDivergenceLogitLoss\nLoss based on the Kullback-Leibler divergence between two categorical distributions.\n\n\nKLDivergenceLoss\nLoss based on the Kullback-Leibler divergence between two categorical distributions.\n\n\nLoss\nBase class for loss functions.\n\n\nMSELoss\nMean squared error loss function.\n\n\nModularityLoss\nDifferentiable relaxation of the Girvan-Newman modularity.\n\n\nMultivariateKurtosis\nMultivariate kurtosis loss for a time series.\n\n\nNormedLoss\n norm regulariser.\n\n\nParameterisedLoss\nExtensible class for loss functions with simple parameterisations.\n\n\nPointAgreementLoss\nLocal point agreement loss.\n\n\nPointHomogeneityLoss\nLocal point homogeneity loss.\n\n\nPointSimilarityLoss\nLocal point similarity loss.\n\n\nQCFCLoss\nQC-FC loss.\n\n\nReferenceTetherLoss\nLoss function penalising distance from a tethered reference point.\n\n\nSecondMomentCentredLoss\nSecond moment loss centred on a precomputed mean.\n\n\nSecondMomentLoss\nSecond moment loss.\n\n\nSmoothnessLoss\nSmoothness loss function.\n\n\nUnilateralLoss\nLoss function corresponding to a single soft nonpositivity constraint.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "nn"
    ]
  },
  {
    "objectID": "api/nn.html#classes",
    "href": "api/nn.html#classes",
    "title": "nn",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBatchCorrelationLoss\nBatch correlation loss.\n\n\nBimodalSymmetricLoss\nLoss based on the minimum distance from either of two modes.\n\n\nBregmanDivergenceLogitLoss\nLoss based on the Bregman divergence between two categorical distributions.\n\n\nBregmanDivergenceLoss\nLoss based on the Bregman divergence between two categorical distributions.\n\n\nCompactnessLoss\nLoss function penalising distances between locations in a mass and the centroid of that mass.\n\n\nConnectopyLoss\nGeneralised connectopic functional, for computing different kinds of connectopic maps.\n\n\nConstraintViolationLoss\nLoss function for constraint violations.\n\n\nDispersionLoss\nLoss function penalising proximity between vectors.\n\n\nEigenmapsLoss\nLaplacian eigenmaps functional.\n\n\nEntropyLogitLoss\nLoss based on the entropy of a categorical distribution.\n\n\nEntropyLoss\nLoss based on the entropy of a categorical distribution.\n\n\nEquilibriumLogitLoss\nMass equilibrium loss.\n\n\nEquilibriumLoss\nMass equilibrium loss.\n\n\nFunctionalHomogeneityLoss\nGlobal functional homogeneity loss.\n\n\nGramDeterminantLoss\nLoss based on the determinant of the Gram matrix.\n\n\nGramLogDeterminantLoss\nLoss based on the log-determinant of the Gram matrix.\n\n\nHingeLoss\nHinge loss function.\n\n\nInterhemisphericTetherLoss\nLoss function penalising distance between matched parcels or objects on opposite hemispheres.\n\n\nJSDivergenceLogitLoss\nLoss based on the Jensen-Shannon divergence between two categorical distributions.\n\n\nJSDivergenceLoss\nLoss based on the Jensen-Shannon divergence between two categorical distributions.\n\n\nKLDivergenceLogitLoss\nLoss based on the Kullback-Leibler divergence between two categorical distributions.\n\n\nKLDivergenceLoss\nLoss based on the Kullback-Leibler divergence between two categorical distributions.\n\n\nLoss\nBase class for loss functions.\n\n\nMSELoss\nMean squared error loss function.\n\n\nModularityLoss\nDifferentiable relaxation of the Girvan-Newman modularity.\n\n\nMultivariateKurtosis\nMultivariate kurtosis loss for a time series.\n\n\nNormedLoss\n norm regulariser.\n\n\nParameterisedLoss\nExtensible class for loss functions with simple parameterisations.\n\n\nPointAgreementLoss\nLocal point agreement loss.\n\n\nPointHomogeneityLoss\nLocal point homogeneity loss.\n\n\nPointSimilarityLoss\nLocal point similarity loss.\n\n\nQCFCLoss\nQC-FC loss.\n\n\nReferenceTetherLoss\nLoss function penalising distance from a tethered reference point.\n\n\nSecondMomentCentredLoss\nSecond moment loss centred on a precomputed mean.\n\n\nSecondMomentLoss\nSecond moment loss.\n\n\nSmoothnessLoss\nSmoothness loss function.\n\n\nUnilateralLoss\nLoss function corresponding to a single soft nonpositivity constraint.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "nn"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.CompactnessLoss.html",
    "href": "api/hypercoil.loss.nn.CompactnessLoss.html",
    "title": "CompactnessLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.CompactnessLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    coor=None\n    radius=100.0\n    norm=2\n    floor=0.0\n    scalarisation=None\n    key=None\n)\nLoss function penalising distances between locations in a mass and the centroid of that mass.\n.. admonition:: Compactness\nThe compactness is defined as\n\n:math:`\\mathbf{1}^\\intercal\\left(A \\circ \\left\\|C - \\frac{AC}{A\\mathbf{1}} \\right\\|_{cols} \\right)\\mathbf{1}`\n\nGiven a coordinate set :math:`C` for the columns of a weight\n:math:`A`, the compactness measures the weighted average norm of the\ndisplacement of each of the weight's entries from its row's centre of\nmass. (The centre of mass is expressed above as\n:math:`\\frac{AC}{A\\mathbf{1}}`).\n\n.. image:: ../_images/compactloss.gif\n    :width: 200\n    :align: center\n\n`In this simulation, the compactness loss is applied with a\nmulti-logit domain mapper and without any other losses or\nregularisations. The weights collapse to compact but unstructured\nregions of the field.`\nPenalising this quantity can promote more compact rows (i.e., concentrate the weight in each row over columns corresponding to coordinates close to the row’s spatial centre of mass).\n.. warning:: This loss can have a large memory footprint, because it requires computing an intermediate tensor with dimensions equal to the number of rows in the weight, multiplied by the number of columns in the weight, multiplied by the dimension of the coordinate space.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ncoor\nTensor\nCoordinates of the spatial locations in each of the columns of X.\nNone\n\n\nradius\nfloat or None (default 100)\nRadius of the spherical manifold on which the coordinates are located. If this is specified as None, it is assumed that the coordinates are in a space induced by the norm parameter.\n100.0\n\n\nnorm\nint, float or 'inf' (default 2)\nThe norm to use to calculate the distance between the centres of mass and the masses. Ignored if radius is specified; in this case, the spherical geodesic distance is used.\n2\n\n\nfloor\nfloat (default 0)\nMinimum distance to be penalised. This can be used to avoid penalising masses that are already very close to the centre of mass.\n0.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "CompactnessLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.CompactnessLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.CompactnessLoss.html#parameters",
    "title": "CompactnessLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ncoor\nTensor\nCoordinates of the spatial locations in each of the columns of X.\nNone\n\n\nradius\nfloat or None (default 100)\nRadius of the spherical manifold on which the coordinates are located. If this is specified as None, it is assumed that the coordinates are in a space induced by the norm parameter.\n100.0\n\n\nnorm\nint, float or 'inf' (default 2)\nThe norm to use to calculate the distance between the centres of mass and the masses. Ignored if radius is specified; in this case, the spherical geodesic distance is used.\n2\n\n\nfloor\nfloat (default 0)\nMinimum distance to be penalised. This can be used to avoid penalising masses that are already very close to the centre of mass.\n0.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "CompactnessLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.SecondMomentCentredLoss.html",
    "href": "api/hypercoil.loss.nn.SecondMomentCentredLoss.html",
    "title": "SecondMomentCentredLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.SecondMomentCentredLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    standardise_data=False\n    standardise_mu=False\n    skip_normalise=False\n    scalarisation=None\n    key=None\n)\nSecond moment loss centred on a precomputed mean.\nGiven an input matrix :math:T and a weight matrix :math:A, the second moment is computed as\n:math:\\left[ A \\circ \\left (T - \\frac{AT}{A\\mathbf{1}} \\right )^2  \\right] \\frac{\\mathbf{1}}{A \\mathbf{1}}\nThe term :math:\\frac{AT}{A\\mathbf{1}} can also be precomputed and passed as the mu argument to the :func:second_moment_centred function. If the mean is already known, it is more efficient to use that function. Otherwise, the :func:second_moment function will compute the mean internally.\nRegularise the second moment, e.g. to favour a dimension reduction mapping that is internally homogeneous.\n.. admonition:: Second Moment\nSecond moment losses are based on a reduction of the second moment\nquantity\n\n:math:`\\left[ A \\circ \\left (T - \\frac{AT}{A\\mathbf{1}} \\right )^2  \\right] \\frac{\\mathbf{1}}{A \\mathbf{1}}`\n\nwhere the division operator is applied elementwise with broadcasting\nand the difference operator is applied via broadcasting. The\nbroadcasting operations involved in the core computation -- estimating\na weighted mean and then computing the weighted sum of squares about\nthat mean -- are illustrated in the below cartoon.\n\n.. image:: ../_images/secondmomentloss.svg\n    :width: 300\n    :align: center\n\n*Illustration of the most memory-intensive stage of loss computation.\nThe lavender tensor represents the weighted mean, the blue tensor the\noriginal observations, and the green tensor the weights (which might\ncorrespond to a dimension reduction mapping such as a parcellation).*\n.. note:: In practice, we’ve found that using the actual second moment loss often results in large and uneven parcels. Accordingly, an unnormalised extension of the second moment (which omits the normalisation :math:\\frac{1}{A \\mathbf{1}}) is also available. This unnormalised quantity is equivalent to the weighted mean squared error about each weighted mean. In practice, we’ve found that this quantity works better for most of our use cases.\n.. warning:: This loss can have a very large memory footprint, because it requires computing an intermediate tensor with dimensions equal to the number of rows in the linear mapping, multiplied by the number of columns in the linear mapping, multiplied by the number of columns in the dataset.\nWhen using this loss to learn a parcellation on voxelwise time series,\nthe full computation will certainly be much too large to fit in GPU\nmemory. Fortunately, because much of the computation is elementwise, it\ncan be broken down along multiple axes without affecting the result.\nThis tensor slicing is implemented automatically in the\n:doc:`ReactiveTerminal &lt;hypercoil.engine.terminal.ReactiveTerminal&gt;`\nclass. Use extreme caution with ``ReactiveTerminals``, as improper use\ncan result in destruction of the computational graph.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nstandardise\n\nIf True, z-score the input matrix before computing the second moment. The default is False.\nrequired\n\n\nskip_normalise\nbool\nIf True, do not include normalisation by the sum of the weights in the computation. In practice, this seems to work better than computing the actual second moment. Instead of computing the second moment, this corresponds to computed a weighted mean squared error about the mean. The default is False.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "SecondMomentCentredLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.SecondMomentCentredLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.SecondMomentCentredLoss.html#parameters",
    "title": "SecondMomentCentredLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nstandardise\n\nIf True, z-score the input matrix before computing the second moment. The default is False.\nrequired\n\n\nskip_normalise\nbool\nIf True, do not include normalisation by the sum of the weights in the computation. In practice, this seems to work better than computing the actual second moment. Instead of computing the second moment, this corresponds to computed a weighted mean squared error about the mean. The default is False.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "SecondMomentCentredLoss"
    ]
  },
  {
    "objectID": "api/activation.html",
    "href": "api/activation.html",
    "title": "activation",
    "section": "",
    "text": "functional.activation\nAdditional activation functions for neural network layers.\n\n\n\n\n\nName\nDescription\n\n\n\n\namplitude_atanh\nInverse hyperbolic tangent (hyperbolic arctangent) activation function applied to the amplitude only.\n\n\namplitude_expbarrier\nExponential barrier activation function applied to the amplitude only.\n\n\namplitude_laplace\nDouble exponential activation function applied to the amplitude only.\n\n\namplitude_tanh\nHyperbolic tangent activation function applied to the amplitude only.\n\n\ncorrnorm\nCorrelation normalisation activation function.\n\n\nexpbarrier\nExponential barrier activation function.\n\n\nisochor\nVolume-normalising activation function for symmetric, positive definite matrices.\n\n\nlaplace\nDouble exponential activation function.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "activation"
    ]
  },
  {
    "objectID": "api/activation.html#functions",
    "href": "api/activation.html#functions",
    "title": "activation",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\namplitude_atanh\nInverse hyperbolic tangent (hyperbolic arctangent) activation function applied to the amplitude only.\n\n\namplitude_expbarrier\nExponential barrier activation function applied to the amplitude only.\n\n\namplitude_laplace\nDouble exponential activation function applied to the amplitude only.\n\n\namplitude_tanh\nHyperbolic tangent activation function applied to the amplitude only.\n\n\ncorrnorm\nCorrelation normalisation activation function.\n\n\nexpbarrier\nExponential barrier activation function.\n\n\nisochor\nVolume-normalising activation function for symmetric, positive definite matrices.\n\n\nlaplace\nDouble exponential activation function.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "activation"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_tanh.html",
    "href": "api/hypercoil.functional.activation.amplitude_tanh.html",
    "title": "amplitude_tanh",
    "section": "",
    "text": "[source]\n\nfunctional.activation.amplitude_tanh(input)\nHyperbolic tangent activation function applied to the amplitude only.\nThe amplitude (absolute value) of the input is transformed according to\n:math:\\mathrm{tanh} x\nwhile the phase (complex argument) is preserved. This function maps the complex plane to the open unit disc: the origin is mapped to itself and distant regions of the complex plane are mapped to the circumference.\n:Dimension: As this activation function is applied elementwise, it conserves dimension; the output will be of the same shape as the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the hyperbolic tangent activation function.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_tanh"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_tanh.html#parameters",
    "href": "api/hypercoil.functional.activation.amplitude_tanh.html#parameters",
    "title": "amplitude_tanh",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the hyperbolic tangent activation function.\nrequired",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_tanh"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_tanh.html#returns",
    "href": "api/hypercoil.functional.activation.amplitude_tanh.html#returns",
    "title": "amplitude_tanh",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_tanh"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.JSDivergenceLogitLoss.html",
    "href": "api/hypercoil.loss.nn.JSDivergenceLogitLoss.html",
    "title": "JSDivergenceLogitLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.JSDivergenceLogitLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    axis=-1\n    keepdims=False\n    reduce=True\n    scalarisation=None\n    key=None\n)\nLoss based on the Jensen-Shannon divergence between two categorical distributions.\nThis operates on logit tensors. For a version that operates on probabilities, see :class:JSDivergenceLoss.\n.. math::\nJS(P || Q) = \\frac{1}{2} KL(P || M) + \\frac{1}{2} KL(Q || M)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the JS divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed JS divergence is computed for each element of the input tensor. Otherwise, the JS divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "JSDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.JSDivergenceLogitLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.JSDivergenceLogitLoss.html#parameters",
    "title": "JSDivergenceLogitLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the JS divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed JS divergence is computed for each element of the input tensor. Otherwise, the JS divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "JSDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ConstraintViolationLoss.html",
    "href": "api/hypercoil.loss.nn.ConstraintViolationLoss.html",
    "title": "ConstraintViolationLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.ConstraintViolationLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    constraints\n    broadcast_against_input=False\n    scalarisation=None\n    key=None\n)\nLoss function for constraint violations.\nThis loss uses a set of constraint functions and evaluates them on its input. If a constraint evaluates to 0 or less, then it is considered to be satisfied and no penalty is applied. Otherwise, a score is returned in proportion to the maximum violation of any constraint.\nFor example, using the identity constraint penalises only positive elements (equivalent to :func:unilateral_loss), while lambda x: -x penalises only negative elements. lambda x : tensor([1, 3, 0, -2]) @ x - 2 applies the specified affine function as a constraint.\n.. warning:: Because of broadcasting rules, the results of constraint computations are not necessarily the same shape as the input, and the output of this function will be the same shape as the largest constraint. This might lead to unexpected scaling of different constraints, and so the broadcast_against_input option is provided to broadcast all outputs against the input shape. In the future, we might add an option that normalises each constraint violation by the number of elements in the output.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nconstraints\nSequence[Callable[[Tensor], Tensor]]\nIterable containing constraint functions.\nrequired\n\n\nbroadcast_against_input\nbool, optional (default: False)\nIf True, broadcast all constraint outputs against the input shape.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ConstraintViolationLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ConstraintViolationLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.ConstraintViolationLoss.html#parameters",
    "title": "ConstraintViolationLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nconstraints\nSequence[Callable[[Tensor], Tensor]]\nIterable containing constraint functions.\nrequired\n\n\nbroadcast_against_input\nbool, optional (default: False)\nIf True, broadcast all constraint outputs against the input shape.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ConstraintViolationLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EquilibriumLoss.html",
    "href": "api/hypercoil.loss.nn.EquilibriumLoss.html",
    "title": "EquilibriumLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.EquilibriumLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    level_axis=-1\n    instance_axes=(-2, -1)\n    scalarisation=None\n    key=None\n)\nMass equilibrium loss.\nThis loss operates on unmapped mass tensors. For a version that operates on logits, see :class:EquilibriumLogitLoss.\nThe equilibrium scores the deviation of the total weight assigned to each parcel or level from the mean weight assigned to each parcel or level. It can be used to encourage the model to learn parcels that are balanced in size.\nLoss functions to favour equal weight across one dimension of a tensor whose slices are masses.\n.. admonition:: Equilibrium\nThe equilibrium loss of a mass tensor :math:`A` is defined as\n\n:math:`\\mathbf{1}^\\intercal \\left[\\left(A \\mathbf{1}\\right) \\circ \\left(A \\mathbf{1}\\right) \\right]`\n\nTh equilibrium loss has applications in the context of parcellation\ntensors. A parcellation tensor is one whose rows correspond to features\n(e.g., voxels, time points, frequency bins, or network nodes) and whose\ncolumns correspond to parcels. Element :math:`i, j` in this tensor\naccordingly indexes the assignment of feature :math:`j` to parcel\n:math:`i`. Examples of parcellation tensors might include atlases that\nmap voxels to regions or affiliation matrices that map graph vertices\nto communities. It is often desirable to constrain feature-parcel\nassignments to :math:`[0, k]` for some :math:`k` and ensure that the\nsum over each feature's assignment is always :math:`k`. (Otherwise, the\nunnormalised loss could be improved by simply shrinking all weights.)\nFor this reason, we can either normalise the loss or situate the\nparcellation tensor in the probability simplex using a\n:doc:`multi-logit (softmax) domain mapper &lt;hypercoil.init.mapparam.ProbabilitySimplexParameter&gt;`.\n\nThe equilibrium loss attains a minimum when parcels are equal in their\ntotal weight. It has a trivial and uninteresting minimum where all\nparcel assignments are equiprobable for all features. Other minima,\nwhich might be of greater interest, occur where each feature is\ndeterministically assigned to a single parcel. These minima can be\nfavoured by using the equilibrium in conjunction with a penalty on the\n:doc:`entropy &lt;hypercoil.loss.entropy&gt;`.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nlevel_axis\nUnion[int, Tuple[int, …]]\nThe axis or axes over which to compute the equilibrium. Within each data instance or weight channel, all elements along the specified axis or axes should correspond to a single level or parcel. The default is -1.\n-1\n\n\nprob_axis\n\nThe axis or axes over which to compute the probabilities (logit version only). The default is -2. In general the union of level_axis and prob_axis should be the same as instance_axes.\nrequired\n\n\ninstance_axes\nUnion[int, Tuple[int, …]]\nThe axis or axes corresponding to a single data instance or weight channel. This should be a superset of level_axis. The default is (-1, -2).\n(-2, -1)\n\n\nkeepdims\n\nAs in :func:jax.numpy.sum. The default is True.\nrequired\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean square scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EquilibriumLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EquilibriumLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.EquilibriumLoss.html#parameters",
    "title": "EquilibriumLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nlevel_axis\nUnion[int, Tuple[int, …]]\nThe axis or axes over which to compute the equilibrium. Within each data instance or weight channel, all elements along the specified axis or axes should correspond to a single level or parcel. The default is -1.\n-1\n\n\nprob_axis\n\nThe axis or axes over which to compute the probabilities (logit version only). The default is -2. In general the union of level_axis and prob_axis should be the same as instance_axes.\nrequired\n\n\ninstance_axes\nUnion[int, Tuple[int, …]]\nThe axis or axes corresponding to a single data instance or weight channel. This should be a superset of level_axis. The default is (-1, -2).\n(-2, -1)\n\n\nkeepdims\n\nAs in :func:jax.numpy.sum. The default is True.\nrequired\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean square scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EquilibriumLoss"
    ]
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API reference",
    "section": "",
    "text": "Functional interfaces for differentiable programming.\n\n\n\nactivation\nAdditional activation functions for neural network layers.\n\n\n\n\n\n\nFunctional and module interfaces for common loss functions.\n\n\n\nnn\nLoss functions as parameterised, callable functional objects.",
    "crumbs": [
      "API reference"
    ]
  },
  {
    "objectID": "api/index.html#functional-functions-and-functionals",
    "href": "api/index.html#functional-functions-and-functionals",
    "title": "API reference",
    "section": "",
    "text": "Functional interfaces for differentiable programming.\n\n\n\nactivation\nAdditional activation functions for neural network layers.",
    "crumbs": [
      "API reference"
    ]
  },
  {
    "objectID": "api/index.html#loss-loss-and-regularisation",
    "href": "api/index.html#loss-loss-and-regularisation",
    "title": "API reference",
    "section": "",
    "text": "Functional and module interfaces for common loss functions.\n\n\n\nnn\nLoss functions as parameterised, callable functional objects.",
    "crumbs": [
      "API reference"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.MSELoss.html",
    "href": "api/hypercoil.loss.nn.MSELoss.html",
    "title": "MSELoss",
    "section": "",
    "text": "[source]\n\nloss.nn.MSELoss(self, nu=1.0, name=None, *, key=None)\nMean squared error loss function.\nAn example of how to compose elements to define a loss function. The score function is the difference between the input and the target, and the scalarisation function is the mean of squared values.\nThere are probably better implementations of the mean squared error loss out there.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "MSELoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.MSELoss.html#parameters",
    "href": "api/hypercoil.loss.nn.MSELoss.html#parameters",
    "title": "MSELoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "MSELoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.Loss.cfg.html",
    "href": "api/hypercoil.loss.nn.Loss.cfg.html",
    "title": "cfg",
    "section": "",
    "text": "cfg\nloss.nn.Loss.cfg(value, where='_nu')\nReturn a copy of the loss function with the specified attribute modified. By default, this modifies the loss multiplier (nu), but this can be changed by specifying the where argument."
  },
  {
    "objectID": "api/hypercoil.loss.nn.EntropyLogitLoss.html",
    "href": "api/hypercoil.loss.nn.EntropyLogitLoss.html",
    "title": "EntropyLogitLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.EntropyLogitLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    axis=-1\n    keepdims=False\n    reduce=True\n    scalarisation=None\n    key=None\n)\nLoss based on the entropy of a categorical distribution.\nThis operates on logit tensors. For a version that operates on probabilities, see :class:EntropyLoss.\n.. admonition:: Entropy\nThe entropy of a categorical distribution :math:`A` is defined as\n\n:math:`-\\mathbf{1}^\\intercal \\left(A \\circ \\log A\\right) \\mathbf{1}`\n\n(where :math:`\\log` denotes the elementwise logarithm).\n\n.. image:: ../_images/entropysimplex.svg\n    :width: 250\n    :align: center\n\n*Cartoon schematic of the contours of an entropy-like function over\ncategorical distributions. The function attains its maximum for the\ndistribution in which all outcomes are equiprobable. The function can\nbecome smaller without bound away from this maximum. The superposed\ntriangle represents the probability simplex. By pre-transforming the\npenalised weights to constrain them to the simplex, the entropy\nfunction is bounded and attains a separate minimum for each\ndeterministic distribution.*\n\nPenalising the entropy promotes concentration of weight into a single\ncategory. This has applications in problem settings such as\nparcellation, when more deterministic parcel assignments are desired.\n.. warning:: Entropy is a concave function. Minimising it without constraint affords an unbounded capacity for reducing the loss. This is almost certainly undesirable. For this reason, it is recommended that some constraint be imposed on the input set when placing a penalty on entropy. One possibility is using a :doc:probability simplex parameter mapper &lt;hypercoil.init.mapparam.ProbabilitySimplexParameter&gt; to first project the input weights onto the probability simplex.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the entropy.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed probability-weighted surprise is computed for each element of the input tensor. Otherwise, the entropy is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EntropyLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EntropyLogitLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.EntropyLogitLoss.html#parameters",
    "title": "EntropyLogitLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the entropy.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed probability-weighted surprise is computed for each element of the input tensor. Otherwise, the entropy is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EntropyLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.isochor.html",
    "href": "api/hypercoil.functional.activation.isochor.html",
    "title": "isochor",
    "section": "",
    "text": "[source]\n\nfunctional.activation.isochor(\n    input\n    volume=1\n    max_condition=None\n    softmax_temp=None\n)\nVolume-normalising activation function for symmetric, positive definite matrices.\nThis activation function first finds the eigendecomposition of each input matrix. The eigenvalues are then each divided by :math:\\sqrt[n]{\\frac{v_{in}}{v_{target}}} to normalise the determinant to :math:v_{target}. Before normalisation, there are options to rescale the eigenvalues through a softmax and/or to enforce a maximal condition number for the output tensor. If the input tensors are being used to represent ellipsoids, for instance, this can constrain the eccentricity of those ellipsoids. Finally, the matrix is reconstituted using the original eigenvectors and the rescaled eigenvalues.\n:Dimension: input : :math:(*, P, P) P denotes the row and column dimensions of the input matrices. * denotes any number of additional dimensions. output : :math:(*, P, P) As above.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\ntensor\nTensor containing symmetric, positive definite matrices.\nrequired\n\n\nvolume\nfloat (default 1)\nTarget volume for the normalisation procedure. All output tensors will have this determinant.\n1\n\n\nmax_condition\nfloat :math:\\in \\[1, \\infty) or None (default None)\nMaximum permissible condition number among output tensors. This can be used to constrain the eccentricity of isochoric ellipsoids. To enforce this maximum, the eigenvalues of the input tensors are replaced with a convex combination of the original eigenvalues and a vector of ones such that the largest eigenvalue is no more than max_condition times the smallest eigenvalue. Note that a max_condition of 1 will always return (a potentially isotropically scaled) identity.\nNone\n\n\nsoftmax_temp\nfloat or None (default None)\nIf this is provided, then the eigenvalues of the input tensor are passed through a softmax with the specified temperature before any other processing.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntensor\nVolume-normalised tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "isochor"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.isochor.html#parameters",
    "href": "api/hypercoil.functional.activation.isochor.html#parameters",
    "title": "isochor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\ntensor\nTensor containing symmetric, positive definite matrices.\nrequired\n\n\nvolume\nfloat (default 1)\nTarget volume for the normalisation procedure. All output tensors will have this determinant.\n1\n\n\nmax_condition\nfloat :math:\\in \\[1, \\infty) or None (default None)\nMaximum permissible condition number among output tensors. This can be used to constrain the eccentricity of isochoric ellipsoids. To enforce this maximum, the eigenvalues of the input tensors are replaced with a convex combination of the original eigenvalues and a vector of ones such that the largest eigenvalue is no more than max_condition times the smallest eigenvalue. Note that a max_condition of 1 will always return (a potentially isotropically scaled) identity.\nNone\n\n\nsoftmax_temp\nfloat or None (default None)\nIf this is provided, then the eigenvalues of the input tensor are passed through a softmax with the specified temperature before any other processing.\nNone",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "isochor"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.isochor.html#returns",
    "href": "api/hypercoil.functional.activation.isochor.html#returns",
    "title": "isochor",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\ntensor\nVolume-normalised tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "isochor"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.SecondMomentLoss.html",
    "href": "api/hypercoil.loss.nn.SecondMomentLoss.html",
    "title": "SecondMomentLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.SecondMomentLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    standardise=False\n    skip_normalise=False\n    scalarisation=None\n    key=None\n)\nSecond moment loss.\nGiven an input matrix :math:T and a weight matrix :math:A, the second moment is computed as\n:math:\\left[ A \\circ \\left (T - \\frac{AT}{A\\mathbf{1}} \\right )^2  \\right] \\frac{\\mathbf{1}}{A \\mathbf{1}}\nThe term :math:\\frac{AT}{A\\mathbf{1}} can also be precomputed and passed as the mu argument to the :func:second_moment_centred function. If the mean is already known, it is more efficient to use that function. Otherwise, the :func:second_moment function will compute the mean internally.\nRegularise the second moment, e.g. to favour a dimension reduction mapping that is internally homogeneous.\n.. admonition:: Second Moment\nSecond moment losses are based on a reduction of the second moment\nquantity\n\n:math:`\\left[ A \\circ \\left (T - \\frac{AT}{A\\mathbf{1}} \\right )^2  \\right] \\frac{\\mathbf{1}}{A \\mathbf{1}}`\n\nwhere the division operator is applied elementwise with broadcasting\nand the difference operator is applied via broadcasting. The\nbroadcasting operations involved in the core computation -- estimating\na weighted mean and then computing the weighted sum of squares about\nthat mean -- are illustrated in the below cartoon.\n\n.. image:: ../_images/secondmomentloss.svg\n    :width: 300\n    :align: center\n\n*Illustration of the most memory-intensive stage of loss computation.\nThe lavender tensor represents the weighted mean, the blue tensor the\noriginal observations, and the green tensor the weights (which might\ncorrespond to a dimension reduction mapping such as a parcellation).*\n.. note:: In practice, we’ve found that using the actual second moment loss often results in large and uneven parcels. Accordingly, an unnormalised extension of the second moment (which omits the normalisation :math:\\frac{1}{A \\mathbf{1}}) is also available. This unnormalised quantity is equivalent to the weighted mean squared error about each weighted mean. In practice, we’ve found that this quantity works better for most of our use cases.\n.. warning:: This loss can have a very large memory footprint, because it requires computing an intermediate tensor with dimensions equal to the number of rows in the linear mapping, multiplied by the number of columns in the linear mapping, multiplied by the number of columns in the dataset.\nWhen using this loss to learn a parcellation on voxelwise time series,\nthe full computation will certainly be much too large to fit in GPU\nmemory. Fortunately, because much of the computation is elementwise, it\ncan be broken down along multiple axes without affecting the result.\nThis tensor slicing is implemented automatically in the\n:doc:`ReactiveTerminal &lt;hypercoil.engine.terminal.ReactiveTerminal&gt;`\nclass. Use extreme caution with ``ReactiveTerminals``, as improper use\ncan result in destruction of the computational graph.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nstandardise\nbool\nIf True, z-score the input matrix before computing the second moment. The default is False.\nFalse\n\n\nskip_normalise\nbool\nIf True, do not include normalisation by the sum of the weights in the computation. In practice, this seems to work better than computing the actual second moment. Instead of computing the second moment, this corresponds to computed a weighted mean squared error about the mean. The default is False.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "SecondMomentLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.SecondMomentLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.SecondMomentLoss.html#parameters",
    "title": "SecondMomentLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nstandardise\nbool\nIf True, z-score the input matrix before computing the second moment. The default is False.\nFalse\n\n\nskip_normalise\nbool\nIf True, do not include normalisation by the sum of the weights in the computation. In practice, this seems to work better than computing the actual second moment. Instead of computing the second moment, this corresponds to computed a weighted mean squared error about the mean. The default is False.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "SecondMomentLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.JSDivergenceLoss.html",
    "href": "api/hypercoil.loss.nn.JSDivergenceLoss.html",
    "title": "JSDivergenceLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.JSDivergenceLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    axis=-1\n    keepdims=False\n    reduce=True\n    scalarisation=None\n    key=None\n)\nLoss based on the Jensen-Shannon divergence between two categorical distributions.\nThis operates on probability tensors. For a version that operates on logits, see :class:JSDivergenceLogitLoss.\n.. math::\nJS(P || Q) = \\frac{1}{2} KL(P || M) + \\frac{1}{2} KL(Q || M)\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the JS divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed JS divergence is computed for each element of the input tensor. Otherwise, the JS divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "JSDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.JSDivergenceLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.JSDivergenceLoss.html#parameters",
    "title": "JSDivergenceLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the JS divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed JS divergence is computed for each element of the input tensor. Otherwise, the JS divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "JSDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BatchCorrelationLoss.html",
    "href": "api/hypercoil.loss.nn.BatchCorrelationLoss.html",
    "title": "BatchCorrelationLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.BatchCorrelationLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    tol=0\n    tol_sig=0.1\n    abs=True\n    scalarisation=None\n    key=None\n)\nBatch correlation loss.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntol\nnonnegative float or \\'auto\\' (default 0)\nTolerance for correlations. Only correlation values above tol are counted. If this is set to 'auto', a tolerance is computed for the batch size given the significance level in tol_sig.\n0\n\n\ntol_sig\nfloat in (0, 1)\nSignificance level for correlation tolerance. Used only if tol is set to 'auto'.\n0.1\n\n\nabs\nbool (default True)\nUse the absolute value of correlations. If this is being used as a loss function, the model’s weights will thus be updated to shrink all batchwise correlations toward zero.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BatchCorrelationLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BatchCorrelationLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.BatchCorrelationLoss.html#parameters",
    "title": "BatchCorrelationLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntol\nnonnegative float or \\'auto\\' (default 0)\nTolerance for correlations. Only correlation values above tol are counted. If this is set to 'auto', a tolerance is computed for the batch size given the significance level in tol_sig.\n0\n\n\ntol_sig\nfloat in (0, 1)\nSignificance level for correlation tolerance. Used only if tol is set to 'auto'.\n0.1\n\n\nabs\nbool (default True)\nUse the absolute value of correlations. If this is being used as a loss function, the model’s weights will thus be updated to shrink all batchwise correlations toward zero.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BatchCorrelationLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.UnilateralLoss.html",
    "href": "api/hypercoil.loss.nn.UnilateralLoss.html",
    "title": "UnilateralLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.UnilateralLoss(self, nu=1.0, name=None, *, scalarisation=None, key=None)\nLoss function corresponding to a single soft nonpositivity constraint.\nThis loss penalises only positive elements of its input. It is a special case of :func:constraint_violation with the identity constraint.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "UnilateralLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.UnilateralLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.UnilateralLoss.html#parameters",
    "title": "UnilateralLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "UnilateralLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.FunctionalHomogeneityLoss.html",
    "href": "api/hypercoil.loss.nn.FunctionalHomogeneityLoss.html",
    "title": "FunctionalHomogeneityLoss",
    "section": "",
    "text": "FunctionalHomogeneityLoss\n\n[source]\n\nloss.nn.FunctionalHomogeneityLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    standardise=False\n    skip_normalise=False\n    use_geom_mean=False\n    use_schaefer=False\n    scalarisation=None\n    key=None\n)\nGlobal functional homogeneity loss.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "FunctionalHomogeneityLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.NormedLoss.html",
    "href": "api/hypercoil.loss.nn.NormedLoss.html",
    "title": "NormedLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.NormedLoss(\n    self\n    nu=1.0\n    name=None\n    score=identity\n    *\n    p=2.0\n    axis=None\n    outer_scalarise=mean_scalarise\n    key=None\n)\n:math:L_p norm regulariser.\nAn example of how to compose elements to define a loss function. By default, this function flattens the input tensor and computes the :math:L_2 norm of the resulting vector. The dimensions to be flattened and the norm order can be specified using the axis and p arguments respectively. If the norm is computed over only a subset of axes, the remaining axes can be further reduced by specifying a scalarisation function using the outer_scalarise argument. By default, the outer scalarisation function is the mean function. Setting this to an identity function will result in a loss function that returns a vector of values for each observation.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscore\nCallable\nThe scoring function to be used to compute the loss value. This function should take a single argument, which is a tensor of arbitrary shape, and return a score value for each (potentially multivariate) observation in the tensor. p: float The order of the norm to be computed. If p = 1, the function computes the :math:L_1 Manhattan / city block norm. If p = 2, the function computes the :math:L_2 Euclidean norm. If p = inf, the function computes the :math:L_\\infty maximum norm.\nidentity\n\n\naxis\nUnion[int, Sequence[int]]\nThe axes to be flattened. If None, all axes are flattened.\nNone\n\n\nouter_scalarise\nCallable\nThe scalarisation function to be applied to any dimensions that are not flattened (i.e., those not specified in axis). If None, the mean function is used. If axis is None, this argument is ignored. To return a vector of values for each observation, explicitly set this to an identity function.\nmean_scalarise",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "NormedLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.NormedLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.NormedLoss.html#parameters",
    "title": "NormedLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscore\nCallable\nThe scoring function to be used to compute the loss value. This function should take a single argument, which is a tensor of arbitrary shape, and return a score value for each (potentially multivariate) observation in the tensor. p: float The order of the norm to be computed. If p = 1, the function computes the :math:L_1 Manhattan / city block norm. If p = 2, the function computes the :math:L_2 Euclidean norm. If p = inf, the function computes the :math:L_\\infty maximum norm.\nidentity\n\n\naxis\nUnion[int, Sequence[int]]\nThe axes to be flattened. If None, all axes are flattened.\nNone\n\n\nouter_scalarise\nCallable\nThe scalarisation function to be applied to any dimensions that are not flattened (i.e., those not specified in axis). If None, the mean function is used. If axis is None, this argument is ignored. To return a vector of values for each observation, explicitly set this to an identity function.\nmean_scalarise",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "NormedLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.KLDivergenceLogitLoss.html",
    "href": "api/hypercoil.loss.nn.KLDivergenceLogitLoss.html",
    "title": "KLDivergenceLogitLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.KLDivergenceLogitLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    axis=-1\n    keepdims=False\n    reduce=True\n    scalarisation=None\n    key=None\n)\nLoss based on the Kullback-Leibler divergence between two categorical distributions.\nThis operates on logit tensors. For a version that operates on probabilities, see :class:KLDivergenceLoss.\nAdapted from distrax.\n.. note::\nThe KL divergence is not symmetric, so this function returns\n:math:`KL(P || Q)`. For a symmetric measure, see\n:func:`js_divergence`.\n.. math::\nKL(P || Q) = \\sum_{x \\in \\mathcal{X}}^n P_x \\log \\frac{P_x}{Q_x}\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the KL divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed KL divergence is computed for each element of the input tensor. Otherwise, the KL divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "KLDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.KLDivergenceLogitLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.KLDivergenceLogitLoss.html#parameters",
    "title": "KLDivergenceLogitLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the KL divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed KL divergence is computed for each element of the input tensor. Otherwise, the KL divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "KLDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_atanh.html",
    "href": "api/hypercoil.functional.activation.amplitude_atanh.html",
    "title": "amplitude_atanh",
    "section": "",
    "text": "[source]\n\nfunctional.activation.amplitude_atanh(input)\nInverse hyperbolic tangent (hyperbolic arctangent) activation function applied to the amplitude only.\nThe amplitude (absolute value) of the input is transformed according to\n:math:\\mathrm{arctanh} x\nwhile the phase (complex argument) is preserved. This function maps the open unit disc in the complex plane into the entire complex plane: the origin is mapped to itself and the circumference is mapped to infinity.\n:Dimension: As this activation function is applied elementwise, it conserves dimension; the output will be of the same shape as the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the hyperbolic arctangent function.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_atanh"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_atanh.html#parameters",
    "href": "api/hypercoil.functional.activation.amplitude_atanh.html#parameters",
    "title": "amplitude_atanh",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the hyperbolic arctangent function.\nrequired",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_atanh"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_atanh.html#returns",
    "href": "api/hypercoil.functional.activation.amplitude_atanh.html#returns",
    "title": "amplitude_atanh",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_atanh"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EigenmapsLoss.html",
    "href": "api/hypercoil.loss.nn.EigenmapsLoss.html",
    "title": "EigenmapsLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.EigenmapsLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    theta=None\n    omega=None\n    scalarisation=None\n    key=None\n)\nLaplacian eigenmaps functional.\n.. warning::\nThis function is provided as an illustrative example of how to\nparameterise the connectopy functional. It is not recommended for\npractical use, because it is incredibly inefficient and numerically\nunstable. Instead, use the ``laplacian_eigenmaps`` function from\n``hypercoil.functional``.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\ntensor, float, or None (default None)\nParameterisation of the pairwise dissimilarity function.\nNone\n\n\nomega\ntensor, float, or None (default None)\nOptional parameterisation of the affinity function, if one is provided.\nNone\n\n\ndissimilarity\ncallable\nFunction to compute dissimilarity between latent coordinates induced by the proposed connectopies. By default, the square of the L2 distance is used. The callable must accept Q and theta as arguments. (theta may be unused.)\nrequired\n\n\naffinity\ncallable or None (default None)\nIf an affinity function is provided, then the image of argument A under this function is the affinity matrix. Otherwise, argument A is the affinity matrix.\nrequired\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EigenmapsLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EigenmapsLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.EigenmapsLoss.html#parameters",
    "title": "EigenmapsLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\ntensor, float, or None (default None)\nParameterisation of the pairwise dissimilarity function.\nNone\n\n\nomega\ntensor, float, or None (default None)\nOptional parameterisation of the affinity function, if one is provided.\nNone\n\n\ndissimilarity\ncallable\nFunction to compute dissimilarity between latent coordinates induced by the proposed connectopies. By default, the square of the L2 distance is used. The callable must accept Q and theta as arguments. (theta may be unused.)\nrequired\n\n\naffinity\ncallable or None (default None)\nIf an affinity function is provided, then the image of argument A under this function is the affinity matrix. Otherwise, argument A is the affinity matrix.\nrequired\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EigenmapsLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.laplace.html",
    "href": "api/hypercoil.functional.activation.laplace.html",
    "title": "laplace",
    "section": "",
    "text": "[source]\n\nfunctional.activation.laplace(input, loc=0, width=1)\nDouble exponential activation function.\nThe double exponential activation function is applied elementwise as\n:math:e^{\\frac{-|x - \\mu|}{b}}\nto inputs x with centre :math:\\mu and width b. It constrains its outputs to the range (0, 1], mapping values closer to its centre to larger outputs. It is Lipschitz continuous over the reals and differentiable except at its centre.\n:Dimension: As this activation function is applied elementwise, it conserves dimension; the output will be of the same shape as the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor to be transformed elementwise by the double exponential activation function.\nrequired\n\n\nloc\nfloat or broadcastable Tensor (default 0)\nCentre parameter :math:\\mu of the double exponential function.\n0\n\n\nwidth\nfloat or broadcastable Tensor (default 1)\nSpread parameter b of the double exponential function.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "laplace"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.laplace.html#parameters",
    "href": "api/hypercoil.functional.activation.laplace.html#parameters",
    "title": "laplace",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor to be transformed elementwise by the double exponential activation function.\nrequired\n\n\nloc\nfloat or broadcastable Tensor (default 0)\nCentre parameter :math:\\mu of the double exponential function.\n0\n\n\nwidth\nfloat or broadcastable Tensor (default 1)\nSpread parameter b of the double exponential function.\n1",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "laplace"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.laplace.html#returns",
    "href": "api/hypercoil.functional.activation.laplace.html#returns",
    "title": "laplace",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "laplace"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ConnectopyLoss.html",
    "href": "api/hypercoil.loss.nn.ConnectopyLoss.html",
    "title": "ConnectopyLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.ConnectopyLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    theta=None\n    omega=None\n    dissimilarity=None\n    affinity=None\n    negative_affinity='reciprocal'\n    scalarisation=None\n    progressive_theta=False\n    key=None\n)\nGeneralised connectopic functional, for computing different kinds of connectopic maps.\n.. admonition:: Connectopic functional\nGiven an affinity matrix A, the connectopic functional is the\nobjective\n\n:math:`\\mathbf{1}^\\intercal \\left( \\mathbf{A} \\circ S_\\theta(\\mathbf{Q}) \\right) \\mathbf{1}`\n\nfor a pairwise function S. The default pairwise function is the square\nof the L2 distance. The columns of the Q that minimises the objective\nare the learned connectopic maps.\n.. warning:: If you’re using this for a well-characterised connectopic map with a closed-form or algorithmically optimised solution, such as Laplacian eigenmaps or many forms of community detection, then in most cases you would be better off directly computing exact maps rather than using this functional to approximate them.\nBecause this operation attempts to learn all of the maps that jointly\nminimise the objective in a single shot rather than using iterative\nprojection, it is more prone to misalignment than a projective approach\nfor eigendecomposition-based maps.\n.. danger:: Note that a connectopic loss is often insufficient on its own as a loss. It should be combined with appropriate projections and constraints, for instance to ensure the learned maps are zero-centred and orthogonal.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\ntensor, float, or None (default None)\nParameterisation of the pairwise dissimilarity function.\nNone\n\n\nomega\ntensor, float, or None (default None)\nOptional parameterisation of the affinity function, if one is provided.\nNone\n\n\ndissimilarity\ncallable\nFunction to compute dissimilarity between latent coordinates induced by the proposed connectopies. By default, the square of the L2 distance is used. The callable must accept Q and theta as arguments. (theta may be unused.)\nNone\n\n\naffinity\ncallable or None (default None)\nIf an affinity function is provided, then the image of argument A under this function is the affinity matrix. Otherwise, argument A is the affinity matrix.\nNone\n\n\nprogressive_theta\nbool (default False)\nWhen this is True, a theta is generated such that the last map in Q has a weight of 1, the second-to-last has a weight of 2, and so on. This can be used to encourage the last column to correspond to the least important connectopic map and the first column to correspond to the most important connectopic map.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ConnectopyLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ConnectopyLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.ConnectopyLoss.html#parameters",
    "title": "ConnectopyLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\ntensor, float, or None (default None)\nParameterisation of the pairwise dissimilarity function.\nNone\n\n\nomega\ntensor, float, or None (default None)\nOptional parameterisation of the affinity function, if one is provided.\nNone\n\n\ndissimilarity\ncallable\nFunction to compute dissimilarity between latent coordinates induced by the proposed connectopies. By default, the square of the L2 distance is used. The callable must accept Q and theta as arguments. (theta may be unused.)\nNone\n\n\naffinity\ncallable or None (default None)\nIf an affinity function is provided, then the image of argument A under this function is the affinity matrix. Otherwise, argument A is the affinity matrix.\nNone\n\n\nprogressive_theta\nbool (default False)\nWhen this is True, a theta is generated such that the last map in Q has a weight of 1, the second-to-last has a weight of 2, and so on. This can be used to encourage the last column to correspond to the least important connectopic map and the first column to correspond to the most important connectopic map.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ConnectopyLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.QCFCLoss.html",
    "href": "api/hypercoil.loss.nn.QCFCLoss.html",
    "title": "QCFCLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.QCFCLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    tol=0\n    tol_sig=0.1\n    abs=True\n    scalarisation=None\n    key=None\n)\nQC-FC loss.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntol\nnonnegative float or \\'auto\\' (default 0)\nTolerance for correlations. Only correlation values above tol are counted. If this is set to 'auto', a tolerance is computed for the batch size given the significance level in tol_sig.\n0\n\n\ntol_sig\nfloat in (0, 1)\nSignificance level for correlation tolerance. Used only if tol is set to 'auto'.\n0.1\n\n\nabs\nbool (default True)\nUse the absolute value of correlations. If this is being used as a loss function, the model’s weights will thus be updated to shrink all batchwise correlations toward zero.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "QCFCLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.QCFCLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.QCFCLoss.html#parameters",
    "title": "QCFCLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntol\nnonnegative float or \\'auto\\' (default 0)\nTolerance for correlations. Only correlation values above tol are counted. If this is set to 'auto', a tolerance is computed for the batch size given the significance level in tol_sig.\n0\n\n\ntol_sig\nfloat in (0, 1)\nSignificance level for correlation tolerance. Used only if tol is set to 'auto'.\n0.1\n\n\nabs\nbool (default True)\nUse the absolute value of correlations. If this is being used as a loss function, the model’s weights will thus be updated to shrink all batchwise correlations toward zero.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "QCFCLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.SmoothnessLoss.html",
    "href": "api/hypercoil.loss.nn.SmoothnessLoss.html",
    "title": "SmoothnessLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.SmoothnessLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    n=1\n    pad_value=None\n    axis=-1\n    scalarisation=None\n    key=None\n)\nSmoothness loss function.\nThis loss penalises large or sudden changes in the input tensor. It is currently a thin wrapper around jax.numpy.diff.\n.. warning::\nThis function returns both positive and negative values, and so\nshould probably not be used with a scalarisation map like\n``mean_scalarise`` or ``sum_scalarise``. Instead, maps like\n``meansq_scalarise`` or ``vnorm_scalarise`` with either the ``p=1``\nor ``p=inf`` options might be more appropriate.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nn\nint\nNumber of times to differentiate using the backwards differences method.\n1\n\n\naxis\nint, optional (default: -1)\nAxis defining the slice of the input tensor over which differences are computed\n-1\n\n\npad_value\nfloat, optional (default: None)\nArguments to jnp.diff. Values to prepend to the input along the specified axis before computing the difference.\nNone\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the L1 norm scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "SmoothnessLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.SmoothnessLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.SmoothnessLoss.html#parameters",
    "title": "SmoothnessLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nn\nint\nNumber of times to differentiate using the backwards differences method.\n1\n\n\naxis\nint, optional (default: -1)\nAxis defining the slice of the input tensor over which differences are computed\n-1\n\n\npad_value\nfloat, optional (default: None)\nArguments to jnp.diff. Values to prepend to the input along the specified axis before computing the difference.\nNone\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the L1 norm scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "SmoothnessLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.MultivariateKurtosis.html",
    "href": "api/hypercoil.loss.nn.MultivariateKurtosis.html",
    "title": "MultivariateKurtosis",
    "section": "",
    "text": "[source]\n\nloss.nn.MultivariateKurtosis(\n    self\n    nu=1.0\n    name=None\n    *\n    l2=0.0\n    dimensional_scaling=False\n    scalarisation=None\n    key=None\n)\nMultivariate kurtosis loss for a time series.\nThis is the multivariate kurtosis following Mardia, as used by Laumann and colleagues in the setting of functional connectivity. It is equal to the mean of the squared Mahalanobis norm of each time point (as parameterised by the inverse covariance of the multivariate time series).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nl2\nfloat (default 0)\nL2 regularisation to be applied to the covariance matrix to ensure that it is invertible.\n0.0\n\n\ndimensional_scaling\nbool (default False)\nThe expected value of the multivariate kurtosis for a normally distributed, stationary process of infinite duration with d channels (or variables) is :math:d (d + 2). Setting this to true normalises for the process dimension by dividing the obtained kurtosis by :math:d (d + 2). This has no effect in determining the optimum.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "MultivariateKurtosis"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.MultivariateKurtosis.html#parameters",
    "href": "api/hypercoil.loss.nn.MultivariateKurtosis.html#parameters",
    "title": "MultivariateKurtosis",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nl2\nfloat (default 0)\nL2 regularisation to be applied to the covariance matrix to ensure that it is invertible.\n0.0\n\n\ndimensional_scaling\nbool (default False)\nThe expected value of the multivariate kurtosis for a normally distributed, stationary process of infinite duration with d channels (or variables) is :math:d (d + 2). Setting this to true normalises for the process dimension by dividing the obtained kurtosis by :math:d (d + 2). This has no effect in determining the optimum.\nFalse\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "MultivariateKurtosis"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.InterhemisphericTetherLoss.html",
    "href": "api/hypercoil.loss.nn.InterhemisphericTetherLoss.html",
    "title": "InterhemisphericTetherLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.InterhemisphericTetherLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    lh_coor=None\n    rh_coor=None\n    radius=100.0\n    scalarisation=None\n    key=None\n)\nLoss function penalising distance between matched parcels or objects on opposite hemispheres.\nDisplacement of centres of mass in one cortical hemisphere from corresponding centres of mass in the other cortical hemisphere.\n.. admonition:: Hemispheric Tether\nThe hemispheric tether is defined as\n\n:math:`\\sum_{\\ell} \\left\\| \\ell_{LH, centre} - \\ell_{RH, centre} ight\\|`\n\nwhere :math:`\\ell` denotes a pair of regions, one in each cortical\nhemisphere.\n\n.. image:: ../_images/spatialnull.gif\n    :width: 500\n    :align: center\n\n`The symmetry of this spatial null model is enforced through a\nmoderately strong hemispheric tether.`\nWhen an atlas is initialised with the same number of parcels in each cortical hemisphere compartment, the hemispheric tether can be used to approximately enforce symmetry and to enforce analogy between a pair of parcels in the two cortical hemispheres.\n.. warning:: Currently, this loss only works in spherical surface space.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nlh_coor\nTensor\nCoordinates of the spatial locations in each of the columns of lh.\nNone\n\n\nrh_coor\nTensor\nCoordinates of the spatial locations in each of the columns of rh.\nNone\n\n\nradius\nfloat (default 100)\nRadius of the spherical manifold on which the coordinates are located.\n100.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "InterhemisphericTetherLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.InterhemisphericTetherLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.InterhemisphericTetherLoss.html#parameters",
    "title": "InterhemisphericTetherLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nlh_coor\nTensor\nCoordinates of the spatial locations in each of the columns of lh.\nNone\n\n\nrh_coor\nTensor\nCoordinates of the spatial locations in each of the columns of rh.\nNone\n\n\nradius\nfloat (default 100)\nRadius of the spherical manifold on which the coordinates are located.\n100.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "InterhemisphericTetherLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_laplace.html",
    "href": "api/hypercoil.functional.activation.amplitude_laplace.html",
    "title": "amplitude_laplace",
    "section": "",
    "text": "[source]\n\nfunctional.activation.amplitude_laplace(input, loc=0, width=1)\nDouble exponential activation function applied to the amplitude only.\nThe amplitude (absolute value) of the input is transformed according to\n:math:e^{\\frac{-|x - \\mu|}{b}}\nwhile the phase (complex argument) is preserved. This function maps the complex plane to the open unit disc: the origin is mapped to the perimeter and distant regions of the complex plane are mapped to the origin. The function varies quickly near the origin (the region of the plane mapped close to the perimeter and small gradient updates could result in large changes in output. Furthermore, the function is completely discontinuous and undefined at the origin (the direction is ambiguous and any point in the circumference is equally valid).\n:Dimension: As this activation function is applied elementwise, it conserves dimension; the output will be of the same shape as the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the double exponential activation function.\nrequired\n\n\nloc\nfloat or broadcastable Tensor (default 0)\nCentre parameter :math:\\mu of the double exponential function.\n0\n\n\nwidth\nfloat or broadcastable Tensor (default 1)\nSpread parameter b of the double exponential function.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_laplace"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_laplace.html#parameters",
    "href": "api/hypercoil.functional.activation.amplitude_laplace.html#parameters",
    "title": "amplitude_laplace",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the double exponential activation function.\nrequired\n\n\nloc\nfloat or broadcastable Tensor (default 0)\nCentre parameter :math:\\mu of the double exponential function.\n0\n\n\nwidth\nfloat or broadcastable Tensor (default 1)\nSpread parameter b of the double exponential function.\n1",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_laplace"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_laplace.html#returns",
    "href": "api/hypercoil.functional.activation.amplitude_laplace.html#returns",
    "title": "amplitude_laplace",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_laplace"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ModularityLoss.html",
    "href": "api/hypercoil.loss.nn.ModularityLoss.html",
    "title": "ModularityLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.ModularityLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    theta=None\n    gamma=1.0\n    exclude_diag=True\n    scalarisation=None\n    key=None\n)\nDifferentiable relaxation of the Girvan-Newman modularity.\nThis relaxation supports non-deterministic assignments of vertices to communities and non-assortative linkages between communities. It reverts to standard behaviour when the inputs it is provided are standard (i.e., deterministic and associative).\n.. admonition:: Girvan-Newman Modularity Relaxation\nThe relaxed modularity loss is defined as the negative sum of all\nentries in the Hadamard (elementwise) product between the modularity\nmatrix and the coaffiliation matrix.\n\n:math:`\\mathcal{L}_Q = -\\nu_Q \\mathbf{1}^\\intercal \\left( B \\circ H \\right) \\mathbf{1}`\n\n.. image:: ../_images/modularityloss.svg\n    :width: 500\n    :align: center\n\n- The modularity matrix :math:`B` is the difference between the\n  observed connectivity matrix :math:`A` and the expected connectivity\n  matrix under some null model that assumes no community structure,\n  :math:`P`: :math:`B = A - \\gamma P`.\n\n  - The community resolution parameter :math:`\\gamma` essentially\n    determines the scale of the community structure that optimises the\n    relaxed modularity loss.\n\n  - By default, we use the Girvan-Newman null model\n    :math:`P_{GN} = \\frac{A \\mathbf{1} \\mathbf{1}^\\intercal A}{\\mathbf{1}^\\intercal A \\mathbf{1}}`,\n    which can be interpreted as the expected weight of connections\n    between each pair of vertices if all existing edges are cut and\n    then randomly rewired.\n\n  - Note that :math:`A \\mathbf{1}` is the in-degree of the adjacency\n    matrix and :math:`\\mathbf{1}^\\intercal A` is its out-degree, and\n    the two are transposes of one another for symmetric :math:`A`.\n    Also note that the denominator\n    :math:`\\mathbf{1}^\\intercal A \\mathbf{1}` is twice the number of\n    edges for an undirected graph.)\n\n- The coaffiliation matrix :math:`H` is calculated as\n  :math:`H = C_{in} L C_{out}^\\intercal`, where\n  :math:`C_{in} \\in \\mathbb{R}^{(v_{in} \\times c)}` and\n  :math:`C_{out} \\in \\mathbb{R}^{(v_{out} \\times c)}` are proposed\n  assignment weights of in-vertices and out-vertices to communities.\n  :math:`L \\in \\mathbb{R}^{c \\times c)}` is the proposed coupling\n  matrix among each pair of communities and defaults to identity to\n  represent associative structure.\n\n  - Note that, when :math:`C_{in} = C_{out}` is deterministically in\n    :math:`\\{0, 1\\}` and :math:`L = I`, this term reduces to the\n    familiar delta-function notation for the true Girvan-Newman\n    modularity.\nPenalising this favours a weight that induces a modular community structure on the input matrix – or, an input matrix whose structure is reasonably accounted for by the proposed community affiliation weights.\n.. warning:: To conform with the network community interpretation of this loss function, parameters representing the community affiliation :math:C and coupling :math:L matrices can be pre-transformed. Mapping the community affiliation matrix :math:C through a :doc:softmax &lt;hypercoil.init.domain.MultiLogit&gt; function along the community axis lends the affiliation matrix the intuitive interpretation of distributions over communities, or a quantification of the uncertainty of each vertex’s community assignment. Similarly, the coupling matrix can be pre-transformed through a :doc:sigmoid &lt;hypercoil.init.domain.Logit&gt; to constrain inter-community couplings to :math:(0, 1). .. note:: Because the community affiliation matrices :math:C induce parcellations, we can regularise them using parcellation losses. For instance, penalising the :doc:entropy &lt;hypercoil.loss.entropy&gt; will promote a solution wherein each node’s community assignment probability distribution is concentrated in a single community. Similarly, using parcel :doc:equilibrium &lt;hypercoil.loss.equilibrium&gt; will favour a solution wherein communities are of similar sizes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\ntensor, float, or None (default None)\nParameterisation of the pairwise dissimilarity function.\nNone\n\n\nomega\ntensor, float, or None (default None)\nOptional parameterisation of the affinity function, if one is provided.\nrequired\n\n\ndissimilarity\ncallable\nFunction to compute dissimilarity between latent coordinates induced by the proposed connectopies. By default, the square of the L2 distance is used. The callable must accept Q and theta as arguments. (theta may be unused.)\nrequired\n\n\naffinity\ncallable or None (default None)\nIf an affinity function is provided, then the image of argument A under this function is the affinity matrix. Otherwise, argument A is the affinity matrix.\nrequired\n\n\ngamma\nfloat (default 1.)\nModularity parameter. Takes the place of the omega argument in connectopy.\n1.0\n\n\nexclude_diag\nbool (default True)\nIf True, then the diagonal of the affinity matrix is set to zero.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ModularityLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ModularityLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.ModularityLoss.html#parameters",
    "title": "ModularityLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\ntensor, float, or None (default None)\nParameterisation of the pairwise dissimilarity function.\nNone\n\n\nomega\ntensor, float, or None (default None)\nOptional parameterisation of the affinity function, if one is provided.\nrequired\n\n\ndissimilarity\ncallable\nFunction to compute dissimilarity between latent coordinates induced by the proposed connectopies. By default, the square of the L2 distance is used. The callable must accept Q and theta as arguments. (theta may be unused.)\nrequired\n\n\naffinity\ncallable or None (default None)\nIf an affinity function is provided, then the image of argument A under this function is the affinity matrix. Otherwise, argument A is the affinity matrix.\nrequired\n\n\ngamma\nfloat (default 1.)\nModularity parameter. Takes the place of the omega argument in connectopy.\n1.0\n\n\nexclude_diag\nbool (default True)\nIf True, then the diagonal of the affinity matrix is set to zero.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ModularityLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.expbarrier.html",
    "href": "api/hypercoil.functional.activation.expbarrier.html",
    "title": "expbarrier",
    "section": "",
    "text": "[source]\n\nfunctional.activation.expbarrier(input, barrier=1)\nExponential barrier activation function.\nThe exponential barrier activation function is applied elementwise as\n:math:b \\sqrt{1 - \\exp{\\frac{-|x|}{b^2}}}\nto inputs x with barrier b. It constrains its outputs to the range [0, b). It is Lipschitz continuous over the reals and differentiable except at its centre.\n:Dimension: As this activation function is applied elementwise, it conserves dimension; the output will be of the same shape as the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor to be transformed elementwise by the exponential barrier activation function.\nrequired\n\n\nbarrier\nfloat or broadcastable Tensor (default 0)\nBarrier parameter b of the exponential function.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "expbarrier"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.expbarrier.html#parameters",
    "href": "api/hypercoil.functional.activation.expbarrier.html#parameters",
    "title": "expbarrier",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor to be transformed elementwise by the exponential barrier activation function.\nrequired\n\n\nbarrier\nfloat or broadcastable Tensor (default 0)\nBarrier parameter b of the exponential function.\n1",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "expbarrier"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.expbarrier.html#returns",
    "href": "api/hypercoil.functional.activation.expbarrier.html#returns",
    "title": "expbarrier",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "expbarrier"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.PointHomogeneityLoss.html",
    "href": "api/hypercoil.loss.nn.PointHomogeneityLoss.html",
    "title": "PointHomogeneityLoss",
    "section": "",
    "text": "PointHomogeneityLoss\n\n[source]\n\nloss.nn.PointHomogeneityLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    standardise=False\n    scalarisation=None\n    key=None\n)\nLocal point homogeneity loss.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "PointHomogeneityLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EntropyLoss.html",
    "href": "api/hypercoil.loss.nn.EntropyLoss.html",
    "title": "EntropyLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.EntropyLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    axis=-1\n    keepdims=False\n    reduce=True\n    scalarisation=None\n    key=None\n)\nLoss based on the entropy of a categorical distribution.\nThis operates on probability tensors. For a version that operates on logits, see :class:EntropyLogitLoss.\n.. admonition:: Entropy\nThe entropy of a categorical distribution :math:`A` is defined as\n\n:math:`-\\mathbf{1}^\\intercal \\left(A \\circ \\log A\\right) \\mathbf{1}`\n\n(where :math:`\\log` denotes the elementwise logarithm).\n\n.. image:: ../_images/entropysimplex.svg\n    :width: 250\n    :align: center\n\n*Cartoon schematic of the contours of an entropy-like function over\ncategorical distributions. The function attains its maximum for the\ndistribution in which all outcomes are equiprobable. The function can\nbecome smaller without bound away from this maximum. The superposed\ntriangle represents the probability simplex. By pre-transforming the\npenalised weights to constrain them to the simplex, the entropy\nfunction is bounded and attains a separate minimum for each\ndeterministic distribution.*\n\nPenalising the entropy promotes concentration of weight into a single\ncategory. This has applications in problem settings such as\nparcellation, when more deterministic parcel assignments are desired.\n.. warning:: Entropy is a concave function. Minimising it without constraint affords an unbounded capacity for reducing the loss. This is almost certainly undesirable. For this reason, it is recommended that some constraint be imposed on the input set when placing a penalty on entropy. One possibility is using a :doc:probability simplex parameter mapper &lt;hypercoil.init.mapparam.ProbabilitySimplexParameter&gt; to first project the input weights onto the probability simplex.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the entropy.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed probability-weighted surprise is computed for each element of the input tensor. Otherwise, the entropy is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EntropyLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EntropyLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.EntropyLoss.html#parameters",
    "title": "EntropyLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the entropy.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed probability-weighted surprise is computed for each element of the input tensor. Otherwise, the entropy is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EntropyLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BimodalSymmetricLoss.step.html",
    "href": "api/hypercoil.loss.nn.BimodalSymmetricLoss.step.html",
    "title": "step",
    "section": "",
    "text": "step\nloss.nn.BimodalSymmetricLoss.step(count=None)\nIf the loss multiplier is a schedule, this will advance the schedule by one step."
  },
  {
    "objectID": "api/hypercoil.loss.nn.DispersionLoss.html",
    "href": "api/hypercoil.loss.nn.DispersionLoss.html",
    "title": "DispersionLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.DispersionLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    metric=linear_distance\n    scalarisation=None\n    key=None\n)\nLoss function penalising proximity between vectors.\nMutual separation among a set of vectors.\n.. admonition:: Vector dispersion\nThe dispersion among a set of vectors :math:`v \\in \\mathcal{V}` is\ndefined as\n\n:math:`\\sum_{i, j} \\mathrm{d}\\left(v_i - v_j\\right)`\n\nfor some measure of separation :math:`\\mathrm{d}`. (It is also\nvalid to use a reduction other than the sum.)\nThis can be used as one half of a clustering loss. Such a clustering loss would promote mutual separation among centroids (between-cluster separation, imposed by the VectorDispersion loss) while also promoting proximity between observations and their closest centroids (within-cluster closeness, for instance using a :doc:norm loss &lt;hypercoil.loss.norm&gt; or :doc:compactness &lt;hypercoil.loss.cmass.Compactness&gt; if the clusters are associated with spatial coordinates).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nmetric\nCallable\nFunction to calculate the distance between centres of mass. This should take either one or two arguments, and should return the distance between each pair of observations.\nlinear_distance\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "DispersionLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.DispersionLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.DispersionLoss.html#parameters",
    "title": "DispersionLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nmetric\nCallable\nFunction to calculate the distance between centres of mass. This should take either one or two arguments, and should return the distance between each pair of observations.\nlinear_distance\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "DispersionLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html",
    "title": "BregmanDivergenceLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.BregmanDivergenceLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    f\n    f_dim\n    scalarisation=None\n    key=None\n)\nLoss based on the Bregman divergence between two categorical distributions.\nThis operates on unmapped tensors. For a version that operates on logits logits, see :class:BregmanDivergenceLogitLoss.\nThis function computes the Bregman divergence between the input tensor and the target tensor, induced according to the convex function f.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nTensor\nInput tensor.\nrequired\n\n\nY\nTensor\nTarget tensor.\nrequired\n\n\nf\nCallable\nConvex function to induce the Bregman divergence.\nrequired\n\n\nf_dim\nint\nDimension of arguments to f.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTensor\nBregman divergence score for each set of observations.\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html#parameters",
    "title": "BregmanDivergenceLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html#parameters-1",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html#parameters-1",
    "title": "BregmanDivergenceLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nTensor\nInput tensor.\nrequired\n\n\nY\nTensor\nTarget tensor.\nrequired\n\n\nf\nCallable\nConvex function to induce the Bregman divergence.\nrequired\n\n\nf_dim\nint\nDimension of arguments to f.\nrequired",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html#returns",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLoss.html#returns",
    "title": "BregmanDivergenceLoss",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTensor\nBregman divergence score for each set of observations.\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.GramLogDeterminantLoss.html",
    "href": "api/hypercoil.loss.nn.GramLogDeterminantLoss.html",
    "title": "GramLogDeterminantLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.GramLogDeterminantLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    op=corr_kernel\n    theta=None\n    psi=0.0\n    xi=0.0\n    scalarisation=None\n    key=None\n)\nLoss based on the log-determinant of the Gram matrix.\nThis function computes the determinant of the Gram matrix of the input tensor, defined according to the kernel function op. The kernel function should always be a positive semi-definite function, and additional arguments are provided to ensure a non-singular (i.e., strictly positive definite) matrix.\n.. admonition:: Log-det-Gram\nThe log-det-Gram loss among a set of vectors :math:`X` is defined as\nthe negative log-determinant of the Gram matrix of those vectors.\n\n:math:`-\\log \\det \\mathbf{K}(X)`\n\n.. image:: ../_images/determinant.svg\n    :width: 250\n    :align: center\n\nPenalising the negative log-determinant of a Gram matrix can\npromote a degree of independence among the vectors being correlated.\nOne example of the log-det-Gram loss is the log-det-corr loss, which penalises the negative log-determinant of the correlation matrix of a set of vectors. This has a number of desirable properties and applications outlined below.\nCorrelation matrices, which occur frequently in time series analysis, have several properties that make them well-suited for loss functions based on the Gram determinant.\nFirst, correlation matrices are positive semidefinite, and accordingly their determinants will always be nonnegative. For positive semidefinite matrices, the log-determinant is a concave function and accordingly has a global maximum that can be identified using convex optimisation methods.\nSecond, correlation matrices are normalised such that their determinant attains a maximum value of 1. This maximum corresponds to an identity correlation matrix, which in turn occurs when the vectors or time series input to the correlation are orthogonal. Thus, a strong determinant-based loss applied to a correlation matrix will seek an orthogonal basis of input vectors.\nIn the parcellation setting, a weaker log-det-corr loss can be leveraged to promote relative independence of parcels. Combined with a :ref:second-moment loss &lt;hypercoil.loss.secondmoment.SecondMoment&gt;, a log-det-corr loss can be interpreted as inducing a clustering: the second moment loss favours internal similarity of clusters, while the log-det-corr loss favours separation of different clusters.\n.. warning:: Determinant-based losses use jax’s determinant functionality, which itself might use the singular value decomposition in certain cases. Differentiation through SVD involves terms whose denominators include the differences between pairs of singular values. Thus, if two singular values of the input matrix are close together, the gradient can become unstable (and undefined if the singular values are identical). A simple :doc:matrix reconditioning &lt;hypercoil.functional.matrix.recondition_eigenspaces&gt; procedure is available for all operations involving the determinant to reduce the occurrence of degenerate eigenvalues.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\nTensor, optional (default: None)\nKernel parameter tensor. If None, then the kernel is assumed to be isotropic.\nNone\n\n\nop\nCallable, optional (default: :func:corr_kernel)\nKernel function. By default, the Pearson correlation kernel is used.\ncorr_kernel\n\n\npsi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If psi &gt; 0, then the kernel matrix is regularised by adding psi to the diagonal. This can be used to ensure that the matrix is strictly positive definite.\n0.0\n\n\nxi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If xi &gt; 0, then the kernel matrix is regularised by stochastically adding samples from a uniform distribution with support :math:\\psi - \\xi, \\xi to the diagonal. This can be used to ensure that the matrix does not have degenerate eigenvalues. If xi &gt; 0, then psi must also be greater than xi and a key must be provided.\n0.0\n\n\nkey\nOptional['jax.random.PRNGKey']\nRandom number generator key. This is only required if xi &gt; 0.\nNone\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "GramLogDeterminantLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.GramLogDeterminantLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.GramLogDeterminantLoss.html#parameters",
    "title": "GramLogDeterminantLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\nTensor, optional (default: None)\nKernel parameter tensor. If None, then the kernel is assumed to be isotropic.\nNone\n\n\nop\nCallable, optional (default: :func:corr_kernel)\nKernel function. By default, the Pearson correlation kernel is used.\ncorr_kernel\n\n\npsi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If psi &gt; 0, then the kernel matrix is regularised by adding psi to the diagonal. This can be used to ensure that the matrix is strictly positive definite.\n0.0\n\n\nxi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If xi &gt; 0, then the kernel matrix is regularised by stochastically adding samples from a uniform distribution with support :math:\\psi - \\xi, \\xi to the diagonal. This can be used to ensure that the matrix does not have degenerate eigenvalues. If xi &gt; 0, then psi must also be greater than xi and a key must be provided.\n0.0\n\n\nkey\nOptional['jax.random.PRNGKey']\nRandom number generator key. This is only required if xi &gt; 0.\nNone\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "GramLogDeterminantLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ReferenceTetherLoss.html",
    "href": "api/hypercoil.loss.nn.ReferenceTetherLoss.html",
    "title": "ReferenceTetherLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.ReferenceTetherLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    ref=None\n    coor=None\n    radius=100.0\n    scalarisation=None\n    key=None\n)\nLoss function penalising distance from a tethered reference point.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training. {spatial_loss_spec}\n1.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ReferenceTetherLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ReferenceTetherLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.ReferenceTetherLoss.html#parameters",
    "title": "ReferenceTetherLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training. {spatial_loss_spec}\n1.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ReferenceTetherLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.Loss.step.html",
    "href": "api/hypercoil.loss.nn.Loss.step.html",
    "title": "step",
    "section": "",
    "text": "step\nloss.nn.Loss.step(count=None)\nIf the loss multiplier is a schedule, this will advance the schedule by one step."
  },
  {
    "objectID": "api/hypercoil.loss.nn.PointSimilarityLoss.html",
    "href": "api/hypercoil.loss.nn.PointSimilarityLoss.html",
    "title": "PointSimilarityLoss",
    "section": "",
    "text": "PointSimilarityLoss\n\n[source]\n\nloss.nn.PointSimilarityLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    rectify_at='auto'\n    scalarisation=None\n    key=None\n)\nLocal point similarity loss.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "PointSimilarityLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BimodalSymmetricLoss.html",
    "href": "api/hypercoil.loss.nn.BimodalSymmetricLoss.html",
    "title": "BimodalSymmetricLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.BimodalSymmetricLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    modes=(0, 1)\n    scalarisation=None\n    key=None\n)\nLoss based on the minimum distance from either of two modes.\nThis function returns a score equal to the absolute difference between each element of the input tensor and whichever of the two specified modes is closer. Penalising this quantity can be used to concentrate weights at two modes, for instance 0 and 1 or -1 and 1.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nmodes\ntuple(float, float)(default(0, 1))\nModes of the loss.\n(0, 1)\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nstep\nIf the loss multiplier is a schedule, this will advance the schedule by one step.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BimodalSymmetricLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BimodalSymmetricLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.BimodalSymmetricLoss.html#parameters",
    "title": "BimodalSymmetricLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nmodes\ntuple(float, float)(default(0, 1))\nModes of the loss.\n(0, 1)\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BimodalSymmetricLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BimodalSymmetricLoss.html#methods",
    "href": "api/hypercoil.loss.nn.BimodalSymmetricLoss.html#methods",
    "title": "BimodalSymmetricLoss",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nstep\nIf the loss multiplier is a schedule, this will advance the schedule by one step.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BimodalSymmetricLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_expbarrier.html",
    "href": "api/hypercoil.functional.activation.amplitude_expbarrier.html",
    "title": "amplitude_expbarrier",
    "section": "",
    "text": "[source]\n\nfunctional.activation.amplitude_expbarrier(input, barrier=1)\nExponential barrier activation function applied to the amplitude only.\nThe amplitude (absolute value) of the input is transformed according to\n:math:b \\sqrt{1 - \\exp{\\frac{-|x|}{b^2}}}\nwhile the phase (complex argument) is preserved. This function maps the complex plane to the open unit disc: the origin is mapped to itself and distant regions of the complex plane are mapped to the circumference.\n:Dimension: As this activation function is applied elementwise, it conserves dimension; the output will be of the same shape as the input.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the double exponential activation function.\nrequired\n\n\nbarrier\nfloat or broadcastable Tensor (default 0)\nBarrier parameter b of the exponential function.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_expbarrier"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_expbarrier.html#parameters",
    "href": "api/hypercoil.functional.activation.amplitude_expbarrier.html#parameters",
    "title": "amplitude_expbarrier",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor whose amplitude is to be transformed elementwise by the double exponential activation function.\nrequired\n\n\nbarrier\nfloat or broadcastable Tensor (default 0)\nBarrier parameter b of the exponential function.\n1",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_expbarrier"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.amplitude_expbarrier.html#returns",
    "href": "api/hypercoil.functional.activation.amplitude_expbarrier.html#returns",
    "title": "amplitude_expbarrier",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\nTensor\nTransformed input tensor.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "amplitude_expbarrier"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.PointAgreementLoss.html",
    "href": "api/hypercoil.loss.nn.PointAgreementLoss.html",
    "title": "PointAgreementLoss",
    "section": "",
    "text": "PointAgreementLoss\n\n[source]\n\nloss.nn.PointAgreementLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    kappa='auto'\n    rectify_agreement=1.0\n    rectify_boundaries='auto'\n    standardise=False\n    rescale_result=False\n    scalarisation=None\n    key=None\n)\nLocal point agreement loss.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "PointAgreementLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EquilibriumLogitLoss.html",
    "href": "api/hypercoil.loss.nn.EquilibriumLogitLoss.html",
    "title": "EquilibriumLogitLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.EquilibriumLogitLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    level_axis=-1\n    prob_axis=-2\n    instance_axes=(-2, -1)\n    scalarisation=None\n    key=None\n)\nMass equilibrium loss.\nThis loss operates on logits. For a version that operates on unmapped mass tensors, see :class:EquilibriumLoss.\nThe equilibrium scores the deviation of the total weight assigned to each parcel or level from the mean weight assigned to each parcel or level. It can be used to encourage the model to learn parcels that are balanced in size.\nLoss functions to favour equal weight across one dimension of a tensor whose slices are masses.\n.. admonition:: Equilibrium\nThe equilibrium loss of a mass tensor :math:`A` is defined as\n\n:math:`\\mathbf{1}^\\intercal \\left[\\left(A \\mathbf{1}\\right) \\circ \\left(A \\mathbf{1}\\right) \\right]`\n\nTh equilibrium loss has applications in the context of parcellation\ntensors. A parcellation tensor is one whose rows correspond to features\n(e.g., voxels, time points, frequency bins, or network nodes) and whose\ncolumns correspond to parcels. Element :math:`i, j` in this tensor\naccordingly indexes the assignment of feature :math:`j` to parcel\n:math:`i`. Examples of parcellation tensors might include atlases that\nmap voxels to regions or affiliation matrices that map graph vertices\nto communities. It is often desirable to constrain feature-parcel\nassignments to :math:`[0, k]` for some :math:`k` and ensure that the\nsum over each feature's assignment is always :math:`k`. (Otherwise, the\nunnormalised loss could be improved by simply shrinking all weights.)\nFor this reason, we can either normalise the loss or situate the\nparcellation tensor in the probability simplex using a\n:doc:`multi-logit (softmax) domain mapper &lt;hypercoil.init.mapparam.ProbabilitySimplexParameter&gt;`.\n\nThe equilibrium loss attains a minimum when parcels are equal in their\ntotal weight. It has a trivial and uninteresting minimum where all\nparcel assignments are equiprobable for all features. Other minima,\nwhich might be of greater interest, occur where each feature is\ndeterministically assigned to a single parcel. These minima can be\nfavoured by using the equilibrium in conjunction with a penalty on the\n:doc:`entropy &lt;hypercoil.loss.entropy&gt;`.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nlevel_axis\nUnion[int, Tuple[int, …]]\nThe axis or axes over which to compute the equilibrium. Within each data instance or weight channel, all elements along the specified axis or axes should correspond to a single level or parcel. The default is -1.\n-1\n\n\nprob_axis\nUnion[int, Tuple[int, …]]\nThe axis or axes over which to compute the probabilities (logit version only). The default is -2. In general the union of level_axis and prob_axis should be the same as instance_axes.\n-2\n\n\ninstance_axes\nUnion[int, Tuple[int, …]]\nThe axis or axes corresponding to a single data instance or weight channel. This should be a superset of level_axis. The default is (-1, -2).\n(-2, -1)\n\n\nkeepdims\n\nAs in :func:jax.numpy.sum. The default is True.\nrequired\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EquilibriumLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.EquilibriumLogitLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.EquilibriumLogitLoss.html#parameters",
    "title": "EquilibriumLogitLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nlevel_axis\nUnion[int, Tuple[int, …]]\nThe axis or axes over which to compute the equilibrium. Within each data instance or weight channel, all elements along the specified axis or axes should correspond to a single level or parcel. The default is -1.\n-1\n\n\nprob_axis\nUnion[int, Tuple[int, …]]\nThe axis or axes over which to compute the probabilities (logit version only). The default is -2. In general the union of level_axis and prob_axis should be the same as instance_axes.\n-2\n\n\ninstance_axes\nUnion[int, Tuple[int, …]]\nThe axis or axes corresponding to a single data instance or weight channel. This should be a superset of level_axis. The default is (-1, -2).\n(-2, -1)\n\n\nkeepdims\n\nAs in :func:jax.numpy.sum. The default is True.\nrequired\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "EquilibriumLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html",
    "title": "BregmanDivergenceLogitLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.BregmanDivergenceLogitLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    f\n    f_dim\n    scalarisation=None\n    key=None\n)\nLoss based on the Bregman divergence between two categorical distributions.\nThis operates on logits. For a version that operates on unmapped probabilities, see :class:BregmanDivergenceLoss.\nThis function computes the Bregman divergence between the input tensor and the target tensor, induced according to the convex function f.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nTensor\nInput tensor.\nrequired\n\n\nY\nTensor\nTarget tensor.\nrequired\n\n\nf\nCallable\nConvex function to induce the Bregman divergence.\nrequired\n\n\nf_dim\nint\nDimension of arguments to f.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTensor\nBregman divergence score for each set of observations.\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html#parameters",
    "title": "BregmanDivergenceLogitLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html#parameters-1",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html#parameters-1",
    "title": "BregmanDivergenceLogitLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nTensor\nInput tensor.\nrequired\n\n\nY\nTensor\nTarget tensor.\nrequired\n\n\nf\nCallable\nConvex function to induce the Bregman divergence.\nrequired\n\n\nf_dim\nint\nDimension of arguments to f.\nrequired",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html#returns",
    "href": "api/hypercoil.loss.nn.BregmanDivergenceLogitLoss.html#returns",
    "title": "BregmanDivergenceLogitLoss",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTensor\nBregman divergence score for each set of observations.\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "BregmanDivergenceLogitLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.Loss.html",
    "href": "api/hypercoil.loss.nn.Loss.html",
    "title": "Loss",
    "section": "",
    "text": "[source]\n\nloss.nn.Loss(self, score, scalarisation, nu=1.0, name=None, *, key=None)\nBase class for loss functions.\nA loss function is the composition of a score function and a scalarisation map (which might itself be the composition of different tensor rank reduction maps). It also includes a multiplier that can be used to scale its contribution to the overall loss. The multiplier is specified using the nu parameter.\nThe API vis-a-vis dimension reduction is subject to change. We will likely make scalarisations more flexible with regard to both compositionality and the number/specification of dimensions they reduce to.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscore\nCallable\nThe scoring function to be used to compute the loss value. This function should take a single argument, which is a tensor of arbitrary shape, and return a score value for each (potentially multivariate) observation in the tensor.\nrequired\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nrequired\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncfg\nReturn a copy of the loss function with the specified attribute modified. By default, this modifies the loss multiplier (nu), but this can be changed by specifying the where argument.\n\n\nstep\nIf the loss multiplier is a schedule, this will advance the schedule by one step.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "Loss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.Loss.html#parameters",
    "href": "api/hypercoil.loss.nn.Loss.html#parameters",
    "title": "Loss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscore\nCallable\nThe scoring function to be used to compute the loss value. This function should take a single argument, which is a tensor of arbitrary shape, and return a score value for each (potentially multivariate) observation in the tensor.\nrequired\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nrequired",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "Loss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.Loss.html#methods",
    "href": "api/hypercoil.loss.nn.Loss.html#methods",
    "title": "Loss",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncfg\nReturn a copy of the loss function with the specified attribute modified. By default, this modifies the loss multiplier (nu), but this can be changed by specifying the where argument.\n\n\nstep\nIf the loss multiplier is a schedule, this will advance the schedule by one step.",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "Loss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.GramDeterminantLoss.html",
    "href": "api/hypercoil.loss.nn.GramDeterminantLoss.html",
    "title": "GramDeterminantLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.GramDeterminantLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    op=corr_kernel\n    theta=None\n    psi=0.0\n    xi=0.0\n    scalarisation=None\n    key=None\n)\nLoss based on the determinant of the Gram matrix.\nThis function computes the determinant of the Gram matrix of the input tensor, defined according to the kernel function op. The kernel function should always be a positive semi-definite function, and additional arguments are provided to ensure a non-singular (i.e., strictly positive definite) matrix.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\nTensor, optional (default: None)\nKernel parameter tensor. If None, then the kernel is assumed to be isotropic.\nNone\n\n\nop\nCallable, optional (default: :func:corr_kernel)\nKernel function. By default, the Pearson correlation kernel is used.\ncorr_kernel\n\n\npsi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If psi &gt; 0, then the kernel matrix is regularised by adding psi to the diagonal. This can be used to ensure that the matrix is strictly positive definite.\n0.0\n\n\nxi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If xi &gt; 0, then the kernel matrix is regularised by stochastically adding samples from a uniform distribution with support :math:\\psi - \\xi, \\xi to the diagonal. This can be used to ensure that the matrix does not have degenerate eigenvalues. If xi &gt; 0, then psi must also be greater than xi and a key must be provided.\n0.0\n\n\nkey\nOptional['jax.random.PRNGKey']\nRandom number generator key. This is only required if xi &gt; 0.\nNone\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "GramDeterminantLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.GramDeterminantLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.GramDeterminantLoss.html#parameters",
    "title": "GramDeterminantLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\ntheta\nTensor, optional (default: None)\nKernel parameter tensor. If None, then the kernel is assumed to be isotropic.\nNone\n\n\nop\nCallable, optional (default: :func:corr_kernel)\nKernel function. By default, the Pearson correlation kernel is used.\ncorr_kernel\n\n\npsi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If psi &gt; 0, then the kernel matrix is regularised by adding psi to the diagonal. This can be used to ensure that the matrix is strictly positive definite.\n0.0\n\n\nxi\nfloat, optional (default: 0.)\nKernel regularisation parameter. If xi &gt; 0, then the kernel matrix is regularised by stochastically adding samples from a uniform distribution with support :math:\\psi - \\xi, \\xi to the diagonal. This can be used to ensure that the matrix does not have degenerate eigenvalues. If xi &gt; 0, then psi must also be greater than xi and a key must be provided.\n0.0\n\n\nkey\nOptional['jax.random.PRNGKey']\nRandom number generator key. This is only required if xi &gt; 0.\nNone\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "GramDeterminantLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ParameterisedLoss.html",
    "href": "api/hypercoil.loss.nn.ParameterisedLoss.html",
    "title": "ParameterisedLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.ParameterisedLoss(\n    self\n    score\n    scalarisation\n    nu=1.0\n    name=None\n    *\n    params=None\n    key=None\n)\nExtensible class for loss functions with simple parameterisations.\nThis class is intended to be used as a base class for loss functions that have a simple parameterisation, i.e. a fixed set of parameters that are passed to the scoring function. The parameters are specified using the params argument, which should be a mapping from parameter names to values. Note that the class is immutable, so the parameters cannot be changed after the class has been instantiated.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscore\nCallable\nThe scoring function to be used to compute the loss value. This function should take a single argument, which is a tensor of arbitrary shape, and return a score value for each (potentially multivariate) observation in the tensor.\nrequired\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nrequired\n\n\nparams\nOptional[Mapping]\nA mapping from parameter names to values. These will be passed to the scoring function when the loss function is called.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ParameterisedLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.ParameterisedLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.ParameterisedLoss.html#parameters",
    "title": "ParameterisedLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscore\nCallable\nThe scoring function to be used to compute the loss value. This function should take a single argument, which is a tensor of arbitrary shape, and return a score value for each (potentially multivariate) observation in the tensor.\nrequired\n\n\nscalarisation\nCallable\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nrequired\n\n\nparams\nOptional[Mapping]\nA mapping from parameter names to values. These will be passed to the scoring function when the loss function is called.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "ParameterisedLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.KLDivergenceLoss.html",
    "href": "api/hypercoil.loss.nn.KLDivergenceLoss.html",
    "title": "KLDivergenceLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.KLDivergenceLoss(\n    self\n    nu=1.0\n    name=None\n    *\n    axis=-1\n    keepdims=False\n    reduce=True\n    scalarisation=None\n    key=None\n)\nLoss based on the Kullback-Leibler divergence between two categorical distributions.\nThis operates on probability tensors. For a version that operates on logits, see :class:KLDivergenceLogitLoss.\nAdapted from distrax.\n.. note::\nThe KL divergence is not symmetric, so this function returns\n:math:`KL(P || Q)`. For a symmetric measure, see\n:func:`js_divergence`.\n.. math::\nKL(P || Q) = \\sum_{x \\in \\mathcal{X}}^n P_x \\log \\frac{P_x}{Q_x}\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the KL divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed KL divergence is computed for each element of the input tensor. Otherwise, the KL divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "KLDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.KLDivergenceLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.KLDivergenceLoss.html#parameters",
    "title": "KLDivergenceLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\naxis\nint or sequence of ints, optional (default: -1)\nAxis or axes over which to compute the KL divergence.\n-1\n\n\nkeepdims\nbool, optional (default: True)\nAs in jax.numpy.sum.\nFalse\n\n\nreduce\nbool, optional (default: True)\nIf this is False, then the unsummed KL divergence is computed for each element of the input tensor. Otherwise, the KL divergence is computed over the specified axis or axes.\nTrue\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the mean scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "KLDivergenceLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.HingeLoss.html",
    "href": "api/hypercoil.loss.nn.HingeLoss.html",
    "title": "HingeLoss",
    "section": "",
    "text": "[source]\n\nloss.nn.HingeLoss(self, nu=1.0, name=None, *, scalarisation=None, key=None)\nHinge loss function.\nThis is the loss function used in support vector machines. It is a special case of :func:constraint_violation or :func:unilateral_loss where the inputs are transformed according to the following:\n.. math::\n1 - Y \\hat{Y}\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the sum scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "HingeLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.loss.nn.HingeLoss.html#parameters",
    "href": "api/hypercoil.loss.nn.HingeLoss.html#parameters",
    "title": "HingeLoss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nname\nOptional[str]\nDesignated name of the loss function. It is not required that this be specified, but it is recommended to ensure that the loss function can be identified in the context of a reporting utilities. If not explicitly specified, the name will be inferred from the class name and the name of the scoring function.\nNone\n\n\nnu\nfloat\nLoss strength multiplier. This is a scalar multiplier that is applied to the loss value before it is returned. This can be used to modulate the relative contributions of different loss functions to the overall loss value. It can also be used to implement a schedule for the loss function, by dynamically adjusting the multiplier over the course of training.\n1.0\n\n\nscalarisation\nOptional[Callable]\nThe scalarisation function to be used to aggregate the values returned by the scoring function. This function should take a single argument, which is a tensor of arbitrary shape, and return a single scalar value. By default, the sum scalarisation is used.\nNone",
    "crumbs": [
      "`loss`: Loss and regularisation",
      "nn",
      "HingeLoss"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.corrnorm.html",
    "href": "api/hypercoil.functional.activation.corrnorm.html",
    "title": "corrnorm",
    "section": "",
    "text": "[source]\n\nfunctional.activation.corrnorm(input, factor=None, gradpath='both')\nCorrelation normalisation activation function.\nDivide each entry :math:A_{ij} of the input matrix :math:A by the product of the signed square roots of the corresponding diagonals:\n:math:\\bar{A}_{ij} = A_{ij} \\frac{\\mathrm{sgn}(A_{ii} A_{jj})}{\\sqrt{A_{ii}}\\sqrt{A_{jj}} + \\epsilon}\nThis default behaviour, which maps a covariance matrix to a Pearson correlation matrix, can be overriden by providing a factor argument (detailed below). This activation function is also similar to a signed version of the normalisation operation for a graph Laplacian matrix.\n:Dimension: input : :math:(*, P, P) P denotes the row and column dimensions of the input matrices. * denotes any number of additional dimensions. output : :math:(*, P, P) As above.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor to be normalised.\nrequired\n\n\nfactor\nTensor, iterable(Tensor, Tensor), or None (default None)\nNormalisation factor. * If this is not explicitly specified, it follows the default behaviour (division by the product of signed square roots.) * If this is a tensor, input is directly divided by the provided tensor. This option is provided mostly for compatibility with non-square inputs. * If this is a pair of tensors, then input is divided by their outer product.\nNone\n\n\ngradpath\nstr \\'input\\' or \\'both\\' (default \\'both\\')\nIf this is set to 'input' and the default normalisation behaviour is used, then gradient will be blocked from flowing backward through the computation of the normalisation factor from the input.\n'both'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTensor\nNormalised input.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "corrnorm"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.corrnorm.html#parameters",
    "href": "api/hypercoil.functional.activation.corrnorm.html#parameters",
    "title": "corrnorm",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nTensor\nTensor to be normalised.\nrequired\n\n\nfactor\nTensor, iterable(Tensor, Tensor), or None (default None)\nNormalisation factor. * If this is not explicitly specified, it follows the default behaviour (division by the product of signed square roots.) * If this is a tensor, input is directly divided by the provided tensor. This option is provided mostly for compatibility with non-square inputs. * If this is a pair of tensors, then input is divided by their outer product.\nNone\n\n\ngradpath\nstr \\'input\\' or \\'both\\' (default \\'both\\')\nIf this is set to 'input' and the default normalisation behaviour is used, then gradient will be blocked from flowing backward through the computation of the normalisation factor from the input.\n'both'",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "corrnorm"
    ]
  },
  {
    "objectID": "api/hypercoil.functional.activation.corrnorm.html#returns",
    "href": "api/hypercoil.functional.activation.corrnorm.html#returns",
    "title": "corrnorm",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTensor\nNormalised input.",
    "crumbs": [
      "`functional`: Functions and functionals",
      "activation",
      "corrnorm"
    ]
  }
]